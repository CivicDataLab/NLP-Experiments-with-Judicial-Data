{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d9ee5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from opennyai import Pipeline\n",
    "from opennyai.utils import Data\n",
    "from opennyai.ner import get_unique_provision_count\n",
    "\n",
    "import urllib\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66bc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cases_cino = pd.read_csv('POCSO_Filter2.csv')['cino'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1df7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all POCSO judgements in Assam\n",
    "judgement_paths_assam = []\n",
    "path =\"Assam\"\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if(file.endswith(\".txt\")):\n",
    "            judgement_paths_assam.append(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18ed9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 50 filtered POCSO judgements in Assam\n",
    "judgements_filtered = []\n",
    "for judgement in judgement_paths_assam:\n",
    "    #Get Case_ID \n",
    "    case_id = judgement.split(r'/')[-1].split('.txt')[0]\n",
    "    \n",
    "    if case_id in filtered_cases_cino:\n",
    "        judgements_filtered.append(judgement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e34863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(judgements_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d361e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da94a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive Summarisation\n",
    "\n",
    "def opennyai_extractive_summary(judgement_path):\n",
    "    judgement_file = open(judgement_path)\n",
    "    judgement_text = judgement_file.read()\n",
    "    #Get Case_ID \n",
    "    case_id = judgement_path.split(r'/')[-1].split('.txt')[0]\n",
    "    \n",
    "    # load your text files directly into this\n",
    "    texts_to_process = [judgement_text]\n",
    "    # create Data object for data  preprocessing before running ML models\n",
    "    data = Data(texts_to_process, preprocessing_nlp_model='en_core_web_trf')\n",
    "    \n",
    "    pipeline = Pipeline(components=['Rhetorical_Role', 'Summarizer'], use_gpu=False, verbose=True,\n",
    "                    summarizer_summary_length=0.0)\n",
    "    \n",
    "    results = pipeline(data)\n",
    "    \n",
    "    json_result_doc_1 = results[0]\n",
    "    summaries_doc_1 = results[0]['summary']\n",
    "    judgement_file.close()\n",
    "    \n",
    "    with open(\"POCSO_Experiment_Judgements/\"+case_id+\"_summary.json\", \"w\") as outfile:\n",
    "        outfile.write(json.dumps(summaries_doc_1, indent=4))\n",
    "    return summaries_doc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4615222d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8573c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASNG010008652018\n",
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:07<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASNG010018122018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.03s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASNG010018122018\n",
      "ASSN010001342017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.55s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "No Provisions found in Decision\n",
      "NER Done\n",
      "ASSN010004782017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.26s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010004782017\n",
      "ASSN010013622017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:24<00:00, 24.15s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:37<00:00, 37.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:14<00:00, 14.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010013622017\n",
      "ASSN010005202019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.72s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010005202019\n",
      "ASSN070002372019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.06s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070002372019\n",
      "ASSN070000452017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.77s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070000452017\n",
      "ASSN070000182017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.57s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070000182017\n",
      "ASSN070002502018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.35s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070002502018\n",
      "ASSN010000912018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.38s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:44<00:00, 44.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010000912018\n",
      "ASSN070003322018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.65s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:02<00:02,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070003322018\n",
      "ASSN010013062017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.13s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010013062017\n",
      "ASSN010002092017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.07s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:17<00:00, 17.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010002092017\n",
      "ASSN010003222017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.67s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:43<00:00, 43.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSN010002382019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.25s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ There was some issue while performing postprocessing for doc id\n",
      "c36ccf3e9937e248b14511da7dcb0edbe05375e361c6684c61a4a27a8686e921.\n",
      "Some of postprocessing info may be absent because of this in doc.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010002382019\n",
      "ASSN010015512017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.73s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Decision\n",
      "NER Done\n",
      "ASSN010011552017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.90s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:24<00:00, 24.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSN010017512018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.69s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:25<00:00, 25.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSN010011242017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.71s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSN070004442019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.14s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070004442019\n",
      "ASSN010000522018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.47s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:02<00:02,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010000522018\n",
      "ASSN010012862017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.57s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:21<00:00, 21.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010012862017\n",
      "ASSN070005512018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070005512018\n",
      "ASSN010004422017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:17<00:00, 17.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN010004422017\n",
      "ASSN070000642019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.66s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSN070000642019\n",
      "ASSV010001962017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.62s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "No Provisions found in Decision\n",
      "NER Done\n",
      "ASSV090001682017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.09s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV090001682017\n",
      "ASSV090001602017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.08s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV090001602017\n",
      "ASSV010003612019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.96s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ There was some issue while performing postprocessing for doc id\n",
      "1a5602f8fe335a781fa0804654816314551eeb4cb01d495f89b455039a0bd1ef.\n",
      "Some of postprocessing info may be absent because of this in doc.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV010003612019\n",
      "ASSV010005512019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:23<00:00, 23.25s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:48<00:00, 48.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:14<00:00, 14.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSV010008752017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.90s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSV010006662017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:21<00:00, 21.60s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:36<00:00, 36.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:12<00:00, 12.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSV010010952017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.33s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSV010003112017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.55s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV010003112017\n",
      "ASSV010012372017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.37s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV010012372017\n",
      "ASSV010013012018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.92s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:30<00:00, 30.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASSV010001942019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.91s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSV010011632017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.31s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASSV010014162017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.80s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ There was some issue while performing postprocessing for doc id\n",
      "8fd94ae592f700586f8611ec9b6233ec4e36dcd080c28fde3b813cdf89b6c1eb.\n",
      "Some of postprocessing info may be absent because of this in doc.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASSV010014162017\n",
      "ASBR010018052018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010018052018\n",
      "ASBR010009782017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.54s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010009782017\n",
      "ASBR010003392018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.24s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010003392018\n",
      "ASBR080002302019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  5.00s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASBR010001322018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.83s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Done\n",
      "ASBR010013262018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Provisions found in Preamble\n",
      "NER Done\n",
      "ASBR010003042019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.75s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:02<00:02,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010003042019\n",
      "ASBR080001152019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR080001152019\n",
      "ASBR010011522019\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.68s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ There was some issue while performing postprocessing for doc id\n",
      "553ad747763a4dd5166912951341a7c667aaaea97d338c2ac8d9b3f1b7120bac.\n",
      "Some of postprocessing info may be absent because of this in doc.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010011522019\n",
      "ASBR010018192018\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.93s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:17<00:00, 17.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010018192018\n",
      "ASBR010014222017\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Loading Rhetorical Role...\u001b[0m\n",
      "\u001b[38;5;4mℹ Rhetorical Roles will use CPU!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Loading Extractive summarizer...\u001b[0m\n",
      "\u001b[38;5;4mℹ Extractive Summarizer will use CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Preprocessing rhetorical role model input!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.41s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with rhetorical role model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Processing documents with extractive summarizer model!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarisation Complete\n",
      "\u001b[38;5;4mℹ Loading NER...\u001b[0m\n",
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Processing documents with Legal NER!!!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not summarise:  ASBR010014222017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for judgement_path in judgements_filtered:\n",
    "    case_id = judgement_path.split(r'/')[-1].split('.txt')[0]\n",
    "    print(case_id)\n",
    "    if 'POCSO_Experiment_Judgements/'+case_id+'_summary.json' in glob.glob(\"POCSO_Experiment_Judgements/*json\"):\n",
    "        pass\n",
    "    else:\n",
    "        summaries_doc_1 = opennyai_extractive_summary(judgement_path)\n",
    "    print('Summarisation Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf7a3b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ NER will run on CPU!\u001b[0m\n",
      "\u001b[38;5;4mℹ Pre-processing will happen on CPU!\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21449/110884375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtexts_to_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msummaries_doc_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_to_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing_nlp_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en_core_web_trf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/opennyai/pipeline.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mner_json_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ner_model_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rr_model_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summarizer_model_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'NER'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self._ner_model_output = self.__ner_extractor__(data, verbose=self.__verbose__,\n\u001b[0m\u001b[1;32m    119\u001b[0m                                                             \u001b[0mdo_sentence_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__do_sentence_level__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                                                             \u001b[0mdo_postprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__do_postprocess__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/opennyai/ner/InLegalNER/InLegalNER.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, do_sentence_level, do_postprocess, mini_batch_size, verbose, statute_shortforms_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing documents with Legal NER!!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mto_process\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             nlp_doc = extract_entities_from_judgment_text(to_process=to_process, legal_nlp=self.nlp,\n\u001b[0m\u001b[1;32m     64\u001b[0m                                                           \u001b[0mdo_sentence_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_sentence_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                                           mini_batch_size=mini_batch_size)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/opennyai/ner/InLegalNER/entity_recognizer_utils.py\u001b[0m in \u001b[0;36mextract_entities_from_judgment_text\u001b[0;34m(to_process, legal_nlp, mini_batch_size, do_sentence_level)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m######### Combine preamble doc & judgement doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mcombined_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_preamble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_judgement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.from_docs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(components=['NER'], use_gpu=False, verbose=True,\n",
    "                   ner_model_name='en_legal_ner_trf',\n",
    "                   ner_mini_batch_size=40000,\n",
    "                   ner_do_sentence_level=True,\n",
    "                   ner_do_postprocess=True,\n",
    "                   ner_statute_shortforms_path='')\n",
    "\n",
    "texts_to_process = [summaries_doc_1['decision']]\n",
    "data = Data(texts_to_process, preprocessing_nlp_model='en_core_web_trf')\n",
    "results = pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "318528b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(\"POCSO_Experiment_Judgements/*json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fa6d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pocso_provisions(text):\n",
    "    pattern = '\\d+ of pocso'\n",
    "    matches = re.findall(pattern, text.lower())\n",
    "    pattern = '\\d+ of posco'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of the pocso'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of the posco'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of prot'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    pattern = '\\d+ of the prot'\n",
    "    for match in re.findall(pattern, text.lower()):    \n",
    "        matches.append(match)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pattern2 = '\\d+'\n",
    "    provisions_found = []\n",
    "    for match in matches:\n",
    "        provisions_found.append(re.findall(pattern2, match))\n",
    "    provisions_found = list(chain.from_iterable(provisions_found))\n",
    "    \n",
    "    return provisions_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45bfe32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "provisions_dfs = []\n",
    "\n",
    "for summary in glob.glob(\"POCSO_Experiment_Judgements/*json\")[:]:\n",
    "    case_id = summary.split('/')[-1].split('_')[0]\n",
    "    with open(summary) as openfile:\n",
    "        # Reading from json file\n",
    "        summaries_doc_1 = json.load(openfile)\n",
    "        \n",
    "    provisions_preamble = find_pocso_provisions(summaries_doc_1['PREAMBLE'])\n",
    "    provisions_decision = find_pocso_provisions(summaries_doc_1['decision'])\n",
    "    try:\n",
    "        provisions_facts = find_pocso_provisions(summaries_doc_1['facts'])\n",
    "    except:\n",
    "        provisions_facts = []\n",
    "    try:\n",
    "        provisions_issue = find_pocso_provisions(summaries_doc_1['issue'])\n",
    "    except:\n",
    "        provisions_issue = []\n",
    "    \n",
    "    provisions_analysis = find_pocso_provisions(summaries_doc_1['ANALYSIS'])\n",
    "    \n",
    "    provisions_df = pd.DataFrame([[case_id],\n",
    "                                  [set(provisions_preamble)],\n",
    "                                  [set(provisions_decision)],\n",
    "                                 [set(provisions_facts)],\n",
    "                                  [set(provisions_issue)],\n",
    "                                  [set(provisions_analysis)]]\n",
    "                                ).T\n",
    "    \n",
    "    provisions_df.columns = ['case_id', 'provision_preamble', 'provision_decision',\n",
    "                             'provisions_facts', 'provisions_issue', 'provisions_analysis']\n",
    "    provisions_dfs.append(provisions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "138c4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(provisions_dfs).reset_index(drop= True).to_csv('provisions_preamble_decision.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising NER\n",
    "from spacy import displacy\n",
    "from opennyai.ner.ner_utils import ner_displacy_option\n",
    "#displacy.serve(ner_doc_1, style='ent',port=8080,options=ner_displacy_option)\n",
    "displacy.render(ner_doc_preamble, style='ent', options=ner_displacy_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe29678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f382c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for summary in glob.glob(\"POCSO_Experiment_Judgements/*json\")[:]:\n",
    "    case_id = summary.split('/')[-1].split('_')[0]\n",
    "    with open(summary) as openfile:\n",
    "        # Reading from json file\n",
    "        summaries_doc_1 = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf54955",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = glob.glob(\"POCSO_Experiment_Judgements/*json\")[1]\n",
    "with open(summary) as openfile:\n",
    "        # Reading from json file\n",
    "    summaries_doc_1 = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "366611d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hearing the noise raised by her daughter, as the people from nearby gathered, the life of her daughter was saved.\n",
      "\n",
      "\n",
      "sft fe 3&\n",
      "Receiving the same, the police registered a case and investigated the matter.\n",
      "After ae a investigation, the police submitted charge-sheet against the accused Jiten Munda u/s 8 of the POCSO Act.\n",
      "3. The accused in due course appeared before this court to face trial.\n",
      "Copies of the relevant documents were furnished to him.\n",
      "After hearing both the sides on the point of charge,\n",
      "taking note of the materials furnished u/s 173 CrPC, as this court found grounds for presuming that the accused person had committed an offence punishable u/s 8 of the \f",
      "Page 2 of 4 Spl. POCSO Case No. 03/2017 POCSO Act, 2012, the charge was accordingly framed against him, which on being read over and explained, the accused pleaded not guilty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summaries_doc_1['facts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9318c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['PREAMBLE', 'facts', 'ANALYSIS', 'decision'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_doc_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2a16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
